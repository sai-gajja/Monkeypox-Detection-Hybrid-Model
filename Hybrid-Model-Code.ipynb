{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced80fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== IMPORTS ======================\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow warnings\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, average_precision_score\n",
    "from lime import lime_image\n",
    "from lime.wrappers.scikit_image import SegmentationAlgorithm\n",
    "from skimage.segmentation import mark_boundaries\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ====================== CUSTOM LAYERS ======================\n",
    "class ChannelAttention(Layer):\n",
    "    def __init__(self, ratio=8, **kwargs):\n",
    "        super(ChannelAttention, self).__init__(**kwargs)\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.channels = input_shape[-1]\n",
    "        self.shared_dense = Dense(self.channels//self.ratio, activation='relu', \n",
    "                                kernel_initializer='he_normal', kernel_regularizer=l2(1e-3))\n",
    "        self.attention_dense = Dense(self.channels, activation='sigmoid', \n",
    "                                   kernel_initializer='he_normal', kernel_regularizer=l2(1e-3))\n",
    "        super(ChannelAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        gap = tf.reduce_mean(inputs, axis=[1,2], keepdims=True)\n",
    "        gmp = tf.reduce_max(inputs, axis=[1,2], keepdims=True)\n",
    "        attention = self.attention_dense(self.shared_dense(gap) + self.shared_dense(gmp))\n",
    "        return inputs * attention\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ChannelAttention, self).get_config()\n",
    "        config.update({'ratio': self.ratio})\n",
    "        return config\n",
    "\n",
    "class GNNFusionLayer(Layer):\n",
    "    def __init__(self, units=128, **kwargs):\n",
    "        super(GNNFusionLayer, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.dense = Dense(self.units, activation='relu', kernel_regularizer=l2(1e-3))\n",
    "        super(GNNFusionLayer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = tf.squeeze(inputs, axis=1)\n",
    "        x = self.dense(x)\n",
    "        return tf.expand_dims(x, axis=1)\n",
    "\n",
    "# ====================== DATA PIPELINE WITHOUT AUGMENTATION ======================\n",
    "def create_datasets(train_dir, valid_dir, img_size=128, batch_size=32):\n",
    "    class_names = sorted([d.name for d in Path(train_dir).iterdir() if d.is_dir()])\n",
    "    class_counts = {i: len(list((Path(train_dir)/name).glob('*'))) \n",
    "                   for i, name in enumerate(class_names)}\n",
    "    median_freq = np.median(list(class_counts.values()))\n",
    "    class_weights = {i: median_freq/count for i, count in class_counts.items()}\n",
    "\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        train_dir,\n",
    "        label_mode='categorical',\n",
    "        image_size=(img_size, img_size),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        valid_dir,\n",
    "        label_mode='categorical',\n",
    "        image_size=(img_size, img_size),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    preprocess_fn = tf.keras.applications.efficientnet.preprocess_input\n",
    "    train_ds = train_ds.map(lambda x, y: (preprocess_fn(x), y),\n",
    "                         num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.map(lambda x, y: (preprocess_fn(x), y),\n",
    "                      num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_ds, val_ds, class_names, class_weights\n",
    "\n",
    "# ====================== REGULARIZED MODEL ARCHITECTURE ======================\n",
    "def build_model(input_shape=(128, 128, 3), num_classes=5):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # CNN Stream with increased regularization\n",
    "    x = Conv2D(32, 3, padding='same', kernel_regularizer=l2(1e-3), name='conv2d_1')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Conv2D(32, 3, kernel_regularizer=l2(1e-3), name='conv2d_2')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    x = MaxPool2D(2)(x)\n",
    "    x = ChannelAttention()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "\n",
    "    x = Conv2D(64, 3, padding='same', kernel_regularizer=l2(1e-3), name='conv2d_3')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Conv2D(64, 3, kernel_regularizer=l2(1e-3), name='conv2d_4')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    x = MaxPool2D(2)(x)\n",
    "    x = ChannelAttention()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "\n",
    "    x = Conv2D(128, 3, padding='same', kernel_regularizer=l2(1e-3), name='conv2d_5')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Conv2D(128, 3, kernel_regularizer=l2(1e-3), name='conv2d_6')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    x = MaxPool2D(2)(x)\n",
    "    cnn_out = ChannelAttention()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    # EfficientNet Stream\n",
    "    effnet = EfficientNetB3(include_top=False, weights='imagenet', input_tensor=inputs)\n",
    "    effnet.trainable = False\n",
    "\n",
    "    # Feature Fusion\n",
    "    cnn_pool = GlobalAveragePooling2D()(cnn_out)\n",
    "    effnet_pool = GlobalAveragePooling2D()(effnet.output)\n",
    "    fused = Concatenate()([cnn_pool, effnet_pool])\n",
    "\n",
    "    # GNN Processing\n",
    "    graph_nodes = Dense(256, kernel_regularizer=l2(1e-3))(fused)\n",
    "    graph_nodes = Reshape((1, 256))(graph_nodes)\n",
    "    \n",
    "    gn_out = GNNFusionLayer(128)(graph_nodes)\n",
    "    gn_out = Dropout(0.4)(gn_out)\n",
    "    gn_out = GNNFusionLayer(64)(gn_out)\n",
    "    gn_out = Dropout(0.4)(gn_out)\n",
    "    gn_out = Flatten()(gn_out)\n",
    "\n",
    "    # Classification Head with higher dropout\n",
    "    x = Dense(512, kernel_regularizer=l2(1e-3))(gn_out)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    x = Dropout(0.6)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs, outputs, name=\"Regularized_Hybrid_CNN_EffNet_GNN\")\n",
    "    return model\n",
    "\n",
    "# ====================== GRAD-CAM ANALYSIS ======================\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    # Create a model that maps the input image to the activations\n",
    "    # of the last conv layer as well as the output predictions\n",
    "    grad_model = Model(\n",
    "        inputs=model.inputs,\n",
    "        outputs=[model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    # Compute the gradient of the top predicted class for our input image\n",
    "    # with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    # This is the gradient of the output neuron (top predicted or chosen)\n",
    "    # with regard to the output feature map of the last conv layer\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "\n",
    "    # This is a vector where each entry is the mean intensity of the gradient\n",
    "    # over a specific feature map channel\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # We multiply each channel in the feature map array\n",
    "    # by \"how important this channel is\" with regard to the top predicted class\n",
    "    # then sum all the channels to obtain the heatmap class activation\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()\n",
    "\n",
    "class GradCAMCallback(Callback):\n",
    "    def __init__(self, image_path, model, img_size=128, last_conv_layer_name='conv2d_6'):\n",
    "        super().__init__()\n",
    "        self.image_path = image_path\n",
    "        self.img_size = img_size\n",
    "        self.last_conv_layer_name = last_conv_layer_name\n",
    "        self._model = model\n",
    "        self.history = []\n",
    "        os.makedirs('gradcam_analysis', exist_ok=True)\n",
    "        \n",
    "        # Load and preprocess sample image\n",
    "        img = tf.keras.preprocessing.image.load_img(image_path, target_size=(img_size, img_size))\n",
    "        self.sample_image = tf.keras.applications.efficientnet.preprocess_input(\n",
    "            tf.keras.preprocessing.image.img_to_array(img)\n",
    "        )\n",
    "        self.sample_image = np.expand_dims(self.sample_image, axis=0)\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model\n",
    "\n",
    "    @model.setter\n",
    "    def model(self, value):\n",
    "        self._model = value\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        try:\n",
    "            # Generate heatmap\n",
    "            heatmap = make_gradcam_heatmap(\n",
    "                self.sample_image, \n",
    "                self.model, \n",
    "                self.last_conv_layer_name\n",
    "            )\n",
    "            \n",
    "            # Rescale heatmap to a range 0-255\n",
    "            heatmap = np.uint8(255 * heatmap)\n",
    "            \n",
    "            # Use jet colormap to colorize heatmap\n",
    "            jet = plt.cm.get_cmap(\"jet\")\n",
    "            \n",
    "            # Use RGB values of the colormap\n",
    "            jet_colors = jet(np.arange(256))[:, :3]\n",
    "            jet_heatmap = jet_colors[heatmap]\n",
    "            \n",
    "            # Create an image with RGB colorized heatmap\n",
    "            jet_heatmap = tf.keras.preprocessing.image.array_to_img(jet_heatmap)\n",
    "            jet_heatmap = jet_heatmap.resize((self.img_size, self.img_size))\n",
    "            jet_heatmap = tf.keras.preprocessing.image.img_to_array(jet_heatmap)\n",
    "            \n",
    "            # Superimpose the heatmap on original image\n",
    "            original_img = tf.keras.preprocessing.image.array_to_img(self.sample_image[0])\n",
    "            original_img = original_img.resize((self.img_size, self.img_size))\n",
    "            original_img = tf.keras.preprocessing.image.img_to_array(original_img)\n",
    "            \n",
    "            superimposed_img = jet_heatmap * 0.4 + original_img\n",
    "            superimposed_img = tf.keras.preprocessing.image.array_to_img(superimposed_img)\n",
    "            \n",
    "            # Get prediction\n",
    "            pred = self.model.predict(self.sample_image, verbose=0)\n",
    "            \n",
    "            # Save visualization\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.imshow(original_img / 255.0)\n",
    "            plt.title(\"Original Image\")\n",
    "            plt.axis('off')\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.imshow(superimposed_img)\n",
    "            plt.title(f\"Grad-CAM (Epoch {epoch+1})\")\n",
    "            plt.axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            filename = f\"gradcam_analysis/{self.model.name}_epoch_{epoch+1}.png\"\n",
    "            plt.savefig(filename)\n",
    "            plt.close()\n",
    "            \n",
    "            # Store in history\n",
    "            self.history.append({\n",
    "                'epoch': epoch+1,\n",
    "                'heatmap': heatmap,\n",
    "                'superimposed_img': superimposed_img,\n",
    "                'prediction': pred\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating Grad-CAM at epoch {epoch+1}: {str(e)}\")\n",
    "\n",
    "# ====================== IMPROVED LIME CALLBACK ======================\n",
    "class LimeExplainer(Callback):\n",
    "    def __init__(self, image_path, class_names, img_size=128, model=None):\n",
    "        super().__init__()\n",
    "        self.class_names = class_names\n",
    "        self.explainer = lime_image.LimeImageExplainer()\n",
    "        self.segmenter = SegmentationAlgorithm('quickshift', kernel_size=1, max_dist=200, ratio=0.2)\n",
    "        self._model = model\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        if not os.path.exists(image_path):\n",
    "            raise FileNotFoundError(f\"Image not found at {image_path}\")\n",
    "            \n",
    "        img = tf.keras.preprocessing.image.load_img(image_path, target_size=(img_size, img_size))\n",
    "        self.sample_image = tf.keras.applications.efficientnet.preprocess_input(\n",
    "            tf.keras.preprocessing.image.img_to_array(img)\n",
    "        )\n",
    "        self.history = []\n",
    "        os.makedirs('lime_explanations', exist_ok=True)\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model\n",
    "\n",
    "    @model.setter\n",
    "    def model(self, value):\n",
    "        self._model = value\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        try:\n",
    "            img_to_explain = (self.sample_image * 0.5 + 0.5).astype(np.uint8)\n",
    "            \n",
    "            def predict_fn(images):\n",
    "                processed = tf.keras.applications.efficientnet.preprocess_input(images.copy())\n",
    "                return self.model.predict(processed, verbose=0)\n",
    "            \n",
    "            explanation = self.explainer.explain_instance(\n",
    "                img_to_explain,\n",
    "                predict_fn,\n",
    "                top_labels=5,\n",
    "                hide_color=0,\n",
    "                num_samples=1000,\n",
    "                segmentation_fn=self.segmenter\n",
    "            )\n",
    "            \n",
    "            temp, mask = explanation.get_image_and_mask(\n",
    "                explanation.top_labels[0],\n",
    "                positive_only=True,\n",
    "                num_features=5,\n",
    "                hide_rest=False\n",
    "            )\n",
    "            \n",
    "            pred = self.model.predict(np.expand_dims(self.sample_image, axis=0), verbose=0)\n",
    "            self.history.append({\n",
    "                'epoch': epoch+1,\n",
    "                'image': temp,\n",
    "                'mask': mask,\n",
    "                'prediction': self.class_names[np.argmax(pred)],\n",
    "                'confidence': float(np.max(pred))\n",
    "            })\n",
    "            \n",
    "            self._save_epoch_visualization(epoch+1, img_to_explain, temp, mask, pred)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating LIME explanation at epoch {epoch+1}: {str(e)}\")\n",
    "\n",
    "    def _save_epoch_visualization(self, epoch, original_img, explanation_img, mask, pred):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(original_img)\n",
    "        plt.title(f\"Original Image (Epoch {epoch})\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mark_boundaries(explanation_img, mask))\n",
    "        plt.title(f\"Pred: {self.class_names[np.argmax(pred)]} ({np.max(pred):.2f})\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = f\"lime_explanations/{self.model.name}_epoch_{epoch}.png\"\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if not self.history:\n",
    "            print(\"No LIME explanations were generated during training\")\n",
    "            return\n",
    "            \n",
    "        plt.figure(figsize=(20, 5*len(self.history)))\n",
    "        for idx, item in enumerate(self.history):\n",
    "            original_img = (self.sample_image * 0.5 + 0.5).astype(np.uint8)\n",
    "            \n",
    "            plt.subplot(len(self.history), 2, 2*idx+1)\n",
    "            plt.imshow(original_img)\n",
    "            plt.title(f\"Original - Epoch {item['epoch']}\")\n",
    "            plt.axis('off')\n",
    "            \n",
    "            plt.subplot(len(self.history), 2, 2*idx+2)\n",
    "            plt.imshow(mark_boundaries(item['image'], item['mask']))\n",
    "            plt.title(f\"Epoch {item['epoch']}: {item['prediction']} ({item['confidence']:.2f})\")\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        summary_filename = f\"lime_explanations/{self.model.name}_summary.png\"\n",
    "        plt.savefig(summary_filename)\n",
    "        plt.close()\n",
    "\n",
    "# ====================== UTILITY FUNCTIONS ======================\n",
    "def save_model_with_architecture(model, base_filename):\n",
    "    architecture_name = model.name\n",
    "    filename = f\"{base_filename}_{architecture_name}.keras\"\n",
    "    model.save(filename)\n",
    "    print(f\"Model saved as: {filename}\")\n",
    "    return filename\n",
    "\n",
    "def save_training_history(history, model):\n",
    "    history_filename = f\"training_history_{model.name}.json\"\n",
    "    with open(history_filename, 'w') as f:\n",
    "        json.dump(history.history, f, indent=4)\n",
    "    print(f\"Training history saved as: {history_filename}\")\n",
    "    return history_filename\n",
    "\n",
    "def save_classification_report(y_true, y_pred, class_names, model):\n",
    "    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
    "    report_filename = f\"classification_report_{model.name}.json\"\n",
    "    with open(report_filename, 'w') as f:\n",
    "        json.dump(report, f, indent=4)\n",
    "    print(f\"Classification report saved as: {report_filename}\")\n",
    "    return report\n",
    "\n",
    "def plot_pr_curves(y_true, y_pred_probs, class_names, model_name):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i in range(len(class_names)):\n",
    "        precision, recall, _ = precision_recall_curve(y_true[:, i], y_pred_probs[:, i])\n",
    "        ap = average_precision_score(y_true[:, i], y_pred_probs[:, i])\n",
    "        plt.plot(recall, precision, lw=2, \n",
    "                 label=f'{class_names[i]} (AP={ap:.2f})')\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Precision-Recall Curves - {model_name}')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'pr_curves_{model_name}.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_training_metrics(history, model_name):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title(f'{model_name} Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(f'{model_name} Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    filename = f\"training_metrics_{model_name}.png\"\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    return filename\n",
    "\n",
    "# ====================== MAIN TRAINING FUNCTION ======================\n",
    "def main():\n",
    "    # Configuration\n",
    "    train_dir = r\"C:\\Users\\G.SAI\\Desktop\\skin_lesion_research_dataset\\Data_set\\Train\"\n",
    "    valid_dir = r\"C:\\Users\\G.SAI\\Desktop\\skin_lesion_research_dataset\\Data_set\\Val\"\n",
    "    img_size = 128\n",
    "    batch_size = 32\n",
    "    epochs = 15 # Increased for early stopping\n",
    "    \n",
    "    # Create datasets\n",
    "    train_ds, val_ds, class_names, class_weights = create_datasets(train_dir, valid_dir, img_size, batch_size)\n",
    "    \n",
    "    # Build model\n",
    "    model = build_model(input_shape=(img_size, img_size, 3), num_classes=len(class_names))\n",
    "    \n",
    "    # Optimizer with lower learning rate\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        CosineDecay(\n",
    "            initial_learning_rate=1e-4,  # Reduced from 3e-4\n",
    "            decay_steps=len(train_ds)*epochs\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Compile with increased label smoothing\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.3),  # Increased smoothing\n",
    "        metrics=['accuracy',\n",
    "                tf.keras.metrics.Precision(name='precision'),\n",
    "                tf.keras.metrics.Recall(name='recall'),\n",
    "                tf.keras.metrics.AUC(name='auc'),\n",
    "                tf.keras.metrics.AUC(name='pr_auc', curve='PR')]  # Added PR AUC\n",
    "    )\n",
    "\n",
    "    # Sample image paths for visualization callbacks\n",
    "    sample_image_path = r\"C:\\Users\\G.SAI\\Desktop\\M_data\\my_data\\train\\Monkeypox\\MKP_09_03_9.jpg\"\n",
    "    \n",
    "    # Enhanced callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_pr_auc', patience=10, mode='max', restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1),\n",
    "        ModelCheckpoint(f'best_model_{model.name}.keras', save_best_only=True, monitor='val_pr_auc', mode='max'),\n",
    "        LimeExplainer(sample_image_path, class_names, img_size, model),\n",
    "        GradCAMCallback(sample_image_path, model, img_size, 'conv2d_6')  # Changed to use the last conv layer in our custom CNN\n",
    "    ]\n",
    "\n",
    "    # Training\n",
    "    print(f\"Starting training for model: {model.name}\")\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_ds,\n",
    "        class_weight=class_weights,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluation\n",
    "    print(\"\\nFinal Evaluation:\")\n",
    "    best_model = tf.keras.models.load_model(f'best_model_{model.name}.keras', custom_objects={\n",
    "        'ChannelAttention': ChannelAttention,\n",
    "        'GNNFusionLayer': GNNFusionLayer\n",
    "    })\n",
    "    results = best_model.evaluate(val_ds, verbose=0)\n",
    "    print(f\"Validation Accuracy: {results[1]:.4f}\")\n",
    "    print(f\"Validation AUC: {results[4]:.4f}\")\n",
    "    print(f\"Validation PR-AUC: {results[5]:.4f}\")\n",
    "\n",
    "    # Generate predictions\n",
    "    y_true = np.concatenate([y for x, y in val_ds], axis=0)\n",
    "    y_pred_probs = best_model.predict(val_ds, verbose=0)\n",
    "    y_pred_labels = np.argmax(y_pred_probs, axis=1)\n",
    "    y_true_labels = np.argmax(y_true, axis=1)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(confusion_matrix(y_true_labels, y_pred_labels), \n",
    "                annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, \n",
    "                yticklabels=class_names)\n",
    "    plt.title(f'Confusion Matrix - {best_model.name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.savefig(f'confusion_matrix_{best_model.name}.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    report = save_classification_report(y_true_labels, y_pred_labels, class_names, best_model)\n",
    "    print(classification_report(y_true_labels, y_pred_labels, target_names=class_names))\n",
    "\n",
    "    # Precision-Recall Curves\n",
    "    plot_pr_curves(y_true, y_pred_probs, class_names, best_model.name)\n",
    "\n",
    "    # Training metrics plots\n",
    "    plot_training_metrics(history, model.name)\n",
    "\n",
    "    # Save models\n",
    "    best_model_path = save_model_with_architecture(best_model, 'best_model')\n",
    "    final_model_path = save_model_with_architecture(model, 'final_model')\n",
    "\n",
    "    # Save training history\n",
    "    history_path = save_training_history(history, model)\n",
    "\n",
    "    print(\"\\nTraining completed. All outputs saved with model architecture names:\")\n",
    "    print(f\"- Best model: {best_model_path}\")\n",
    "    print(f\"- Final model: {final_model_path}\")\n",
    "    print(f\"- Training metrics: training_metrics_{model.name}.png\")\n",
    "    print(f\"- Confusion matrix: confusion_matrix_{best_model.name}.png\")\n",
    "    print(f\"- PR curves: pr_curves_{best_model.name}.png\")\n",
    "    print(f\"- Classification report: classification_report_{best_model.name}.json\")\n",
    "    print(f\"- Training history: {history_path}\")\n",
    "    print(f\"- LIME explanations saved in lime_explanations/ directory\")\n",
    "    print(f\"- Grad-CAM analysis saved in gradcam_analysis/ directory\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
